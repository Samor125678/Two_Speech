from tkinter import *
from tkinter import scrolledtext
from tkinter import filedialog
import face_recognition
import cv2
import numpy as np
from os import path
import queue
import threading
from google.cloud import speech_v1p1beta1 as speech
import speech_recognition as sr
from threading import Thread,Event
from subprocess import call


def MicRecognize():

    r = sr.Recognizer()

    with sr.Microphone() as source:
        print("Say something!")

        audio = r.listen(source)

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)
        # `
        txt.delete(1.0, END)
        txt.insert(INSERT, "Вы сказали: " + r.recognize_google(audio, language="ru-RU"))
        print("Google Speech Recognition thinks you said " + r.recognize_google(audio, language="ru-RU"))

    except sr.UnknownValueError:

        txt.delete(1.0, END)
        txt.insert(INSERT, "Google Speech Recognition не смог разобрать аудиозапись")
        print("Google Speech Recognition could not understand audio")


    except sr.RequestError as e:

        txt.delete(1.0, END)
        txt.insert(INSERT, "Could not request results from Google Speech Recognition service; {0}".format(e))
        print("Could not request results from Google Speech Recognition service; {0}".format(e))
        #rec = "Could not request results from Google Speech Recognition service; {0}".format(e)


def AudioTranscribe(file_name):
    AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), file_name)
    # AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), "french.aiff")
    # AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), "chinese.flac")

    # use the audio file as the audio source
    r = sr.Recognizer()

    with sr.AudioFile(AUDIO_FILE) as source:
        audio = r.record(source)  # read the entire audio file

    # recognize speech using Google Speech Recognition
    try:
        # for testing purposes, we're just using the default API key
        # to use another API key, use `r.recognize_google(audio, key="GOOGLE_SPEECH_RECOGNITION_API_KEY")`
        # instead of `r.recognize_google(audio)`
        #print("Google Speech Recognition thinks you said " + r.recognize_google(audio, language="ru-RU"))
        recognized = r.recognize_google(audio, language="ru-RU")
        txt.delete(1.0, END)
        txt.insert(INSERT, recognized)
    except sr.UnknownValueError:
        txt.insert(INSERT, "Google Speech Recognition не смог разобрать аудиозапись")
    except sr.RequestError as e:
        txt.insert(INSERT, "Could not request results from Google Speech Recognition service; {0}".format(e))


def Multichannel(file_name):

    from google.cloud import speech_v1p1beta1 as speech
    client = speech.SpeechClient()

    speech_file = file_name

    with open(speech_file, 'rb') as audio_file:
        content = audio_file.read()

    audio = speech.types.RecognitionAudio(content=content)

    config = speech.types.RecognitionConfig(
        encoding=speech.enums.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=8000,
        language_code='en-US',
        audio_channel_count=1,
        enable_separate_recognition_per_channel=True)

    response = client.recognize(config, audio)
    txt.delete(1.0, END)
    for i, result in enumerate(response.results):
        alternative = result.alternatives[0]

        txt.insert(INSERT, '-'*20)
        txt.insert(INSERT, '\n')
        txt.insert(INSERT, 'First alternative of result {}'.format(i))
        txt.insert(INSERT, '\n')
        txt.insert(INSERT, u'Transcript: {}'.format(alternative.transcript))
        txt.insert(INSERT, '\n')
        txt.insert(INSERT, u'Channel Tag: {}'.format(result.channel_tag))
        txt.insert(INSERT, '\n')

    # [END speech_transcribe_multichannel_beta]


def AudioTranscribeMultichannel():

    class AudioRecognition(threading.Thread):
        def __init__(self):
            threading.Thread.__init__(self)
            self.r = sr.Recognizer()
            self.my_queue = queue.Queue()
            self.client = speech.SpeechClient()
            self.r.pause_threshold = 5
            self.config = speech.types.RecognitionConfig(
                language_code='ru-RU',
                audio_channel_count=1,
                enable_separate_recognition_per_channel=True)

        def run(self):
            self.recognise_and_write_in_file()

        def recognise_and_write_in_file(self):
            while True:
                with sr.Microphone() as source:

                    audio = self.r.listen(source)
                    self.my_queue.put(audio.get_wav_data())
                with open("text_name.txt", 'a') as f:
                    # try:
                    txt.insert(INSERT,self.my_queue.empty())
                    txt.delete(1.0, END)
                    print(self.my_queue.empty())
                    audio = speech.types.RecognitionAudio(content=self.my_queue.get())
                    txt.insert(INSERT, "Start recogn")
                    print("Start recogn")
                    response = self.client.recognize(self.config, audio)

                    for i, result in enumerate(response.results):
                        alternative = result.alternatives[0]

                        txt.insert(INSERT, u'Transcript: {}\n'.format(alternative.transcript))
                        txt.insert(INSERT, '\n')
                        txt.insert(INSERT, u'Channel Tag: {}\n'.format(result.channel_tag))
                        txt.insert(INSERT, '\n')

                        f.writelines(u'Transcript: {}\n'.format(alternative.transcript))
                        f.writelines(u'Channel Tag: {}\n'.format(result.channel_tag))

    audio_recognition = AudioRecognition()

    audio_recognition.start()


def Video():
    # Get a reference to webcam #0 (the default one)
    video_capture = cv2.VideoCapture(0)

    # Load a sample picture and learn how to recognize it.
    me = face_recognition.load_image_file("34.jpg")
    me_face_encoding = face_recognition.face_encodings(me)[0]

    # Load a second sample picture and learn how to recognize it.
    biden_image = face_recognition.load_image_file("Alex1.jpg")
    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]

    elvira_image = face_recognition.load_image_file("qwert.jpg")
    elvira_face_encoding = face_recognition.face_encodings(elvira_image)[0]


    # Create arrays of known face encodings and their names
    known_face_encodings = [
        me_face_encoding,
        biden_face_encoding,
        elvira_face_encoding
    ]
    known_face_names = [
        "Farida",
        "Joe Biden",
        "Elvira"
    ]

    # Initialize some variables
    face_locations = []
    face_encodings = []
    face_names = []
    process_this_frame = True


    while True:

        # Grab a single frame of video
        _, frame = video_capture.read()

        # Resize frame of video to 1/4 size for faster face recognition processing
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)

        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)
        rgb_frame = small_frame[:, :, ::-1]

        # Only process every other frame of video to save time
        if process_this_frame:
            # Find all the faces and face encodings in the current frame of video
            face_locations = face_recognition.face_locations(rgb_frame)
            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

            face_names = []
            for face_encoding in face_encodings:
                # See if the face is a match for the known face(s)
                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
                name = "Unknown"

                # # If a match was found in known_face_encodings, just use the first one.
                # if True in matches:
                #     first_match_index = matches.index(True)
                #     name = known_face_names[first_match_index]

                # Or instead, use the known face with the smallest distance to the new face
                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = known_face_names[best_match_index]

                face_names.append(name)
            # print(name)

        process_this_frame = not process_this_frame

        # Display the results
        for (top, right, bottom, left), name in zip(face_locations, face_names):
            # Scale back up face locations since the frame we detected in was scaled to 1/4 size
            top *= 4
            right *= 4
            bottom *= 4
            left *= 4

            # Draw a box around the face
            cv2.rectangle(frame, (left, top), (right, bottom), (100, 44, 155), 2)

            # Draw a label with a name below the face
            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
            font = cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)

        # Display the resulting image
        cv2.imshow('Video', frame)

        # Hit 'q' on the keyboard to quit!
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Release handle to the webcam
    video_capture.release()
    cv2.destroyAllWindows()


class Controller(object):
    def __init__(self):
        self.thread1 = None
        self.thread2 = None
        self.stop_threads = Event()

    # def loop1(self):
    #     call(["python Img.py"], shell=True)
    #
    # def loop2(self):
    #     call(["python Mic.py"], shell=True)

    def loop1(self):
        call(["python Img.py"], shell=True)

    def loop2(self):
        call(["python Mic.py"], shell=True)

    def combine(self):
        self.stop_threads.clear()
        self.thread1 = Thread(target=self.loop1)
        self.thread2 = Thread(target=self.loop2)
        self.thread1.start()
        self.thread2.start()

    def stop(self):
        self.stop_threads.set()
        self.thread1.join()
        self.thread2.join()
        self.thread1 = None
        self.thread2 = None

#Распознавание аудио и видео в режиме онлайн(без разделения говорящих)
def clicked_button1():
    control = Controller()
    control.combine()


#Распознавание по  видео
def clicked_button3():
    Video()


#Распознавание аудио(без разделения говорящих)
def clicked_button4():
    txt.delete(1.0, END)
    txt.insert(INSERT, "Скажите что-нибудь!")
    MicRecognize()


#Распознавание аудио(с разделением говорящих)
def clicked_button5():
    txt.delete(1.0, END)
    txt.insert(INSERT, "Скажите что-нибудь!")
    AudioTranscribeMultichannel()


#Распознавание аудио с готового аудиофайла в формате WAV(без разделения говорящих)
def clicked_button6():
    window.filename = filedialog.askopenfilename(filetypes=(("Audio", "*.WAV"), ("all files", "*.*")))
    AudioTranscribe(window.filename)


#Распознавание аудио с готового аудиофайла в формате WAV(с разделением говорящих)
def clicked_button7():
    window.filename = filedialog.askopenfilename(filetypes=(("Audio", "*.WAV"), ("all files", "*.*")))
    Multichannel(window.filename)



window = Tk()
window.geometry('900x500')
window.title("Добро пожаловать в приложение")

lbl = Label(window, text="Выберите режим")
lbl.grid(column=0, row=3)

txt = scrolledtext.ScrolledText(window, width=100, height=10, bg="#e6e6e6")
txt.grid(column=0, row=300)
txt.insert(INSERT, 'Здесь будет отображаться текст')


btn1 = Button(window, text="Распознавание аудио и видео в режиме онлайн(без разделения говорящих)", command=clicked_button1)
btn1.grid(column=0, row=6, sticky=W)

btn3 = Button(window, text="Распознавание по  видео", command=clicked_button3)
btn3.grid(column=0, row=10, sticky=W)

btn4 = Button(window, text="Распознавание аудио(без разделения говорящих)", command=clicked_button4)
btn4.grid(column=0, row=12, sticky=W)

btn5 = Button(window, text="Распознавание аудио(с разделением говорящих)", command=clicked_button5)
btn5.grid(column=0, row=14, sticky=W)

btn6 = Button(window, text="Распознавание аудио с готового аудиофайла в формате WAV(без разделения говорящих)", command=clicked_button6)
btn6.grid(column=0, row=16, sticky=W)

btn7 = Button(window, text="Распознавание аудио с готового аудиофайла в формате WAV(с разделением говорящих)", command=clicked_button7)
btn7.grid(column=0, row=18, sticky=W)

quitButton = Button(window, text="Выход", command=quit)
quitButton.grid(column=3, row=400, sticky=N)


window.mainloop()